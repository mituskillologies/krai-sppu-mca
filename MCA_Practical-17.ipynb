{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a program to implement GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', \n",
    "                              optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.601331, acc.: 56.25%] [G loss: 0.755036]\n",
      "1 [D loss: 0.341992, acc.: 89.06%] [G loss: 0.882794]\n",
      "2 [D loss: 0.317921, acc.: 92.19%] [G loss: 0.803109]\n",
      "3 [D loss: 0.300681, acc.: 95.31%] [G loss: 0.925084]\n",
      "4 [D loss: 0.286585, acc.: 93.75%] [G loss: 1.148622]\n",
      "5 [D loss: 0.244700, acc.: 98.44%] [G loss: 1.232759]\n",
      "6 [D loss: 0.226621, acc.: 98.44%] [G loss: 1.368230]\n",
      "7 [D loss: 0.166200, acc.: 100.00%] [G loss: 1.647458]\n",
      "8 [D loss: 0.127071, acc.: 100.00%] [G loss: 1.805698]\n",
      "9 [D loss: 0.111605, acc.: 100.00%] [G loss: 1.934219]\n",
      "10 [D loss: 0.096640, acc.: 100.00%] [G loss: 2.038352]\n",
      "11 [D loss: 0.090014, acc.: 100.00%] [G loss: 2.237486]\n",
      "12 [D loss: 0.086920, acc.: 100.00%] [G loss: 2.404849]\n",
      "13 [D loss: 0.062887, acc.: 100.00%] [G loss: 2.476311]\n",
      "14 [D loss: 0.060403, acc.: 100.00%] [G loss: 2.564882]\n",
      "15 [D loss: 0.057259, acc.: 100.00%] [G loss: 2.735839]\n",
      "16 [D loss: 0.039368, acc.: 100.00%] [G loss: 2.825410]\n",
      "17 [D loss: 0.046266, acc.: 100.00%] [G loss: 2.948808]\n",
      "18 [D loss: 0.041714, acc.: 100.00%] [G loss: 2.866207]\n",
      "19 [D loss: 0.032200, acc.: 100.00%] [G loss: 3.135728]\n",
      "20 [D loss: 0.035485, acc.: 100.00%] [G loss: 3.035600]\n",
      "21 [D loss: 0.032140, acc.: 100.00%] [G loss: 3.208318]\n",
      "22 [D loss: 0.027576, acc.: 100.00%] [G loss: 3.186890]\n",
      "23 [D loss: 0.021422, acc.: 100.00%] [G loss: 3.155593]\n",
      "24 [D loss: 0.030074, acc.: 100.00%] [G loss: 3.297783]\n",
      "25 [D loss: 0.031207, acc.: 100.00%] [G loss: 3.392354]\n",
      "26 [D loss: 0.029798, acc.: 100.00%] [G loss: 3.383334]\n",
      "27 [D loss: 0.022646, acc.: 100.00%] [G loss: 3.466522]\n",
      "28 [D loss: 0.019527, acc.: 100.00%] [G loss: 3.581936]\n",
      "29 [D loss: 0.022849, acc.: 100.00%] [G loss: 3.638649]\n",
      "30 [D loss: 0.019020, acc.: 100.00%] [G loss: 3.521166]\n",
      "31 [D loss: 0.019546, acc.: 100.00%] [G loss: 3.543696]\n",
      "32 [D loss: 0.017721, acc.: 100.00%] [G loss: 3.572551]\n",
      "33 [D loss: 0.019317, acc.: 100.00%] [G loss: 3.571920]\n",
      "34 [D loss: 0.015009, acc.: 100.00%] [G loss: 3.660920]\n",
      "35 [D loss: 0.016694, acc.: 100.00%] [G loss: 3.862470]\n",
      "36 [D loss: 0.019711, acc.: 100.00%] [G loss: 3.889047]\n",
      "37 [D loss: 0.014863, acc.: 100.00%] [G loss: 3.731378]\n",
      "38 [D loss: 0.016717, acc.: 100.00%] [G loss: 3.922186]\n",
      "39 [D loss: 0.015574, acc.: 100.00%] [G loss: 4.002995]\n",
      "40 [D loss: 0.014562, acc.: 100.00%] [G loss: 3.991529]\n",
      "41 [D loss: 0.013515, acc.: 100.00%] [G loss: 4.077449]\n",
      "42 [D loss: 0.012074, acc.: 100.00%] [G loss: 4.106639]\n",
      "43 [D loss: 0.014222, acc.: 100.00%] [G loss: 3.882278]\n",
      "44 [D loss: 0.012782, acc.: 100.00%] [G loss: 4.087134]\n",
      "45 [D loss: 0.011978, acc.: 100.00%] [G loss: 4.134489]\n",
      "46 [D loss: 0.012212, acc.: 100.00%] [G loss: 3.989082]\n",
      "47 [D loss: 0.012632, acc.: 100.00%] [G loss: 4.040123]\n",
      "48 [D loss: 0.008737, acc.: 100.00%] [G loss: 4.172809]\n",
      "49 [D loss: 0.013352, acc.: 100.00%] [G loss: 4.125777]\n",
      "50 [D loss: 0.010780, acc.: 100.00%] [G loss: 4.096648]\n",
      "51 [D loss: 0.012389, acc.: 100.00%] [G loss: 4.054023]\n",
      "52 [D loss: 0.012550, acc.: 100.00%] [G loss: 4.227901]\n",
      "53 [D loss: 0.011344, acc.: 100.00%] [G loss: 4.119195]\n",
      "54 [D loss: 0.010953, acc.: 100.00%] [G loss: 4.227337]\n",
      "55 [D loss: 0.014639, acc.: 100.00%] [G loss: 4.243386]\n",
      "56 [D loss: 0.012051, acc.: 100.00%] [G loss: 4.155363]\n",
      "57 [D loss: 0.012354, acc.: 100.00%] [G loss: 4.412790]\n",
      "58 [D loss: 0.011740, acc.: 100.00%] [G loss: 4.230094]\n",
      "59 [D loss: 0.010280, acc.: 100.00%] [G loss: 4.471120]\n",
      "60 [D loss: 0.011182, acc.: 100.00%] [G loss: 4.323606]\n",
      "61 [D loss: 0.007270, acc.: 100.00%] [G loss: 4.355481]\n",
      "62 [D loss: 0.008166, acc.: 100.00%] [G loss: 4.438401]\n",
      "63 [D loss: 0.010959, acc.: 100.00%] [G loss: 4.436276]\n",
      "64 [D loss: 0.011194, acc.: 100.00%] [G loss: 4.398084]\n",
      "65 [D loss: 0.012065, acc.: 100.00%] [G loss: 4.387773]\n",
      "66 [D loss: 0.009061, acc.: 100.00%] [G loss: 4.522814]\n",
      "67 [D loss: 0.009026, acc.: 100.00%] [G loss: 4.557401]\n",
      "68 [D loss: 0.011800, acc.: 100.00%] [G loss: 4.666955]\n",
      "69 [D loss: 0.009686, acc.: 100.00%] [G loss: 4.549992]\n",
      "70 [D loss: 0.007069, acc.: 100.00%] [G loss: 4.508538]\n",
      "71 [D loss: 0.009021, acc.: 100.00%] [G loss: 4.580132]\n",
      "72 [D loss: 0.007055, acc.: 100.00%] [G loss: 4.403297]\n",
      "73 [D loss: 0.011483, acc.: 100.00%] [G loss: 4.452217]\n",
      "74 [D loss: 0.012205, acc.: 100.00%] [G loss: 4.549078]\n",
      "75 [D loss: 0.014630, acc.: 100.00%] [G loss: 4.651523]\n",
      "76 [D loss: 0.008105, acc.: 100.00%] [G loss: 4.728946]\n",
      "77 [D loss: 0.008670, acc.: 100.00%] [G loss: 4.669211]\n",
      "78 [D loss: 0.006654, acc.: 100.00%] [G loss: 4.593053]\n",
      "79 [D loss: 0.008959, acc.: 100.00%] [G loss: 4.536309]\n",
      "80 [D loss: 0.008875, acc.: 100.00%] [G loss: 4.681527]\n",
      "81 [D loss: 0.008986, acc.: 100.00%] [G loss: 4.658558]\n",
      "82 [D loss: 0.008447, acc.: 100.00%] [G loss: 4.601811]\n",
      "83 [D loss: 0.008390, acc.: 100.00%] [G loss: 4.658729]\n",
      "84 [D loss: 0.008561, acc.: 100.00%] [G loss: 4.613783]\n",
      "85 [D loss: 0.010738, acc.: 100.00%] [G loss: 4.752245]\n",
      "86 [D loss: 0.010865, acc.: 100.00%] [G loss: 4.920754]\n",
      "87 [D loss: 0.006064, acc.: 100.00%] [G loss: 4.858536]\n",
      "88 [D loss: 0.011616, acc.: 100.00%] [G loss: 4.982857]\n",
      "89 [D loss: 0.009474, acc.: 100.00%] [G loss: 4.825178]\n",
      "90 [D loss: 0.006934, acc.: 100.00%] [G loss: 4.659007]\n",
      "91 [D loss: 0.012532, acc.: 100.00%] [G loss: 4.716152]\n",
      "92 [D loss: 0.007822, acc.: 100.00%] [G loss: 4.815540]\n",
      "93 [D loss: 0.011076, acc.: 100.00%] [G loss: 4.843187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 [D loss: 0.009124, acc.: 100.00%] [G loss: 4.980047]\n",
      "95 [D loss: 0.006286, acc.: 100.00%] [G loss: 4.798145]\n",
      "96 [D loss: 0.007999, acc.: 100.00%] [G loss: 4.698143]\n",
      "97 [D loss: 0.011667, acc.: 100.00%] [G loss: 4.846433]\n",
      "98 [D loss: 0.013071, acc.: 100.00%] [G loss: 4.912218]\n",
      "99 [D loss: 0.020968, acc.: 100.00%] [G loss: 5.119825]\n",
      "100 [D loss: 0.011652, acc.: 100.00%] [G loss: 4.932281]\n",
      "101 [D loss: 0.009947, acc.: 100.00%] [G loss: 5.047807]\n",
      "102 [D loss: 0.007587, acc.: 100.00%] [G loss: 4.972034]\n",
      "103 [D loss: 0.007730, acc.: 100.00%] [G loss: 5.038666]\n",
      "104 [D loss: 0.012563, acc.: 100.00%] [G loss: 5.048196]\n",
      "105 [D loss: 0.010209, acc.: 100.00%] [G loss: 5.040359]\n",
      "106 [D loss: 0.023420, acc.: 100.00%] [G loss: 4.984288]\n",
      "107 [D loss: 0.019397, acc.: 100.00%] [G loss: 5.119132]\n",
      "108 [D loss: 0.007763, acc.: 100.00%] [G loss: 5.058549]\n",
      "109 [D loss: 0.009767, acc.: 100.00%] [G loss: 4.942173]\n",
      "110 [D loss: 0.013074, acc.: 100.00%] [G loss: 4.833463]\n",
      "111 [D loss: 0.012981, acc.: 100.00%] [G loss: 4.814841]\n",
      "112 [D loss: 0.017299, acc.: 100.00%] [G loss: 5.096800]\n",
      "113 [D loss: 0.009157, acc.: 100.00%] [G loss: 4.815089]\n",
      "114 [D loss: 0.012986, acc.: 100.00%] [G loss: 4.990651]\n",
      "115 [D loss: 0.018111, acc.: 100.00%] [G loss: 5.116452]\n",
      "116 [D loss: 0.023244, acc.: 100.00%] [G loss: 4.019638]\n",
      "117 [D loss: 0.059838, acc.: 98.44%] [G loss: 5.110373]\n",
      "118 [D loss: 0.009860, acc.: 100.00%] [G loss: 5.980501]\n",
      "119 [D loss: 0.366592, acc.: 84.38%] [G loss: 3.799309]\n",
      "120 [D loss: 0.078952, acc.: 96.88%] [G loss: 4.993061]\n",
      "121 [D loss: 0.017987, acc.: 100.00%] [G loss: 5.572997]\n",
      "122 [D loss: 0.031883, acc.: 100.00%] [G loss: 5.334092]\n",
      "123 [D loss: 0.067658, acc.: 98.44%] [G loss: 5.716253]\n",
      "124 [D loss: 0.185043, acc.: 93.75%] [G loss: 4.786425]\n",
      "125 [D loss: 0.055879, acc.: 96.88%] [G loss: 5.943360]\n",
      "126 [D loss: 1.000513, acc.: 65.62%] [G loss: 3.163049]\n",
      "127 [D loss: 0.834915, acc.: 71.88%] [G loss: 3.381197]\n",
      "128 [D loss: 0.053203, acc.: 100.00%] [G loss: 4.353892]\n",
      "129 [D loss: 0.068222, acc.: 96.88%] [G loss: 4.687761]\n",
      "130 [D loss: 0.035893, acc.: 100.00%] [G loss: 4.746057]\n",
      "131 [D loss: 0.038602, acc.: 100.00%] [G loss: 4.238760]\n",
      "132 [D loss: 0.066584, acc.: 98.44%] [G loss: 4.631004]\n",
      "133 [D loss: 0.064235, acc.: 98.44%] [G loss: 4.729104]\n",
      "134 [D loss: 0.057679, acc.: 100.00%] [G loss: 4.399063]\n",
      "135 [D loss: 0.038678, acc.: 100.00%] [G loss: 3.980439]\n",
      "136 [D loss: 0.070430, acc.: 96.88%] [G loss: 4.184968]\n",
      "137 [D loss: 0.269052, acc.: 85.94%] [G loss: 3.930685]\n",
      "138 [D loss: 0.071771, acc.: 98.44%] [G loss: 4.210067]\n",
      "139 [D loss: 0.175595, acc.: 92.19%] [G loss: 3.578120]\n",
      "140 [D loss: 0.057856, acc.: 96.88%] [G loss: 4.090517]\n",
      "141 [D loss: 0.091329, acc.: 98.44%] [G loss: 3.495711]\n",
      "142 [D loss: 0.074046, acc.: 98.44%] [G loss: 3.672240]\n",
      "143 [D loss: 0.067564, acc.: 100.00%] [G loss: 3.488506]\n",
      "144 [D loss: 0.097541, acc.: 96.88%] [G loss: 3.927138]\n",
      "145 [D loss: 0.189200, acc.: 92.19%] [G loss: 3.607136]\n",
      "146 [D loss: 0.069164, acc.: 100.00%] [G loss: 4.224221]\n",
      "147 [D loss: 0.577445, acc.: 79.69%] [G loss: 2.658618]\n",
      "148 [D loss: 0.192502, acc.: 92.19%] [G loss: 3.820522]\n",
      "149 [D loss: 0.084979, acc.: 98.44%] [G loss: 4.757998]\n",
      "150 [D loss: 0.261661, acc.: 92.19%] [G loss: 2.996725]\n",
      "151 [D loss: 0.188527, acc.: 89.06%] [G loss: 4.621965]\n",
      "152 [D loss: 0.151155, acc.: 93.75%] [G loss: 3.851809]\n",
      "153 [D loss: 0.136393, acc.: 93.75%] [G loss: 4.189128]\n",
      "154 [D loss: 0.083352, acc.: 100.00%] [G loss: 4.461646]\n",
      "155 [D loss: 0.206723, acc.: 89.06%] [G loss: 4.497554]\n",
      "156 [D loss: 0.241861, acc.: 89.06%] [G loss: 4.464531]\n",
      "157 [D loss: 0.319591, acc.: 82.81%] [G loss: 3.933166]\n",
      "158 [D loss: 0.078051, acc.: 100.00%] [G loss: 3.995445]\n",
      "159 [D loss: 0.258115, acc.: 89.06%] [G loss: 3.682753]\n",
      "160 [D loss: 0.068538, acc.: 98.44%] [G loss: 3.920011]\n",
      "161 [D loss: 0.137065, acc.: 95.31%] [G loss: 2.958877]\n",
      "162 [D loss: 0.092553, acc.: 95.31%] [G loss: 3.897508]\n",
      "163 [D loss: 0.243603, acc.: 89.06%] [G loss: 3.506659]\n",
      "164 [D loss: 0.044570, acc.: 100.00%] [G loss: 4.298730]\n",
      "165 [D loss: 0.274047, acc.: 89.06%] [G loss: 3.803701]\n",
      "166 [D loss: 0.216394, acc.: 90.62%] [G loss: 4.244328]\n",
      "167 [D loss: 0.938720, acc.: 57.81%] [G loss: 1.454402]\n",
      "168 [D loss: 0.281417, acc.: 85.94%] [G loss: 3.043864]\n",
      "169 [D loss: 0.071866, acc.: 100.00%] [G loss: 4.173522]\n",
      "170 [D loss: 0.167514, acc.: 95.31%] [G loss: 3.013133]\n",
      "171 [D loss: 0.095101, acc.: 96.88%] [G loss: 3.071562]\n",
      "172 [D loss: 0.062486, acc.: 98.44%] [G loss: 3.801221]\n",
      "173 [D loss: 0.169537, acc.: 96.88%] [G loss: 3.312897]\n",
      "174 [D loss: 0.098783, acc.: 96.88%] [G loss: 4.142616]\n",
      "175 [D loss: 0.244112, acc.: 92.19%] [G loss: 3.173460]\n",
      "176 [D loss: 0.129209, acc.: 96.88%] [G loss: 5.158587]\n",
      "177 [D loss: 0.785221, acc.: 67.19%] [G loss: 2.247335]\n",
      "178 [D loss: 0.319861, acc.: 81.25%] [G loss: 3.888173]\n",
      "179 [D loss: 0.074654, acc.: 96.88%] [G loss: 5.345549]\n",
      "180 [D loss: 0.378398, acc.: 84.38%] [G loss: 2.330404]\n",
      "181 [D loss: 0.144777, acc.: 90.62%] [G loss: 3.041365]\n",
      "182 [D loss: 0.095836, acc.: 95.31%] [G loss: 4.223273]\n",
      "183 [D loss: 0.157615, acc.: 96.88%] [G loss: 3.565648]\n",
      "184 [D loss: 0.109397, acc.: 98.44%] [G loss: 4.065246]\n",
      "185 [D loss: 0.226231, acc.: 92.19%] [G loss: 3.359378]\n",
      "186 [D loss: 0.151613, acc.: 95.31%] [G loss: 4.360668]\n",
      "187 [D loss: 0.582917, acc.: 70.31%] [G loss: 2.666638]\n",
      "188 [D loss: 0.080962, acc.: 100.00%] [G loss: 4.300864]\n",
      "189 [D loss: 0.176439, acc.: 95.31%] [G loss: 3.181917]\n",
      "190 [D loss: 0.107121, acc.: 98.44%] [G loss: 3.637481]\n",
      "191 [D loss: 0.209021, acc.: 92.19%] [G loss: 4.648886]\n",
      "192 [D loss: 0.334682, acc.: 85.94%] [G loss: 2.255054]\n",
      "193 [D loss: 0.154234, acc.: 95.31%] [G loss: 4.317871]\n",
      "194 [D loss: 0.288475, acc.: 90.62%] [G loss: 2.890252]\n",
      "195 [D loss: 0.113874, acc.: 98.44%] [G loss: 3.731670]\n",
      "196 [D loss: 0.272280, acc.: 90.62%] [G loss: 3.698488]\n",
      "197 [D loss: 0.375167, acc.: 81.25%] [G loss: 5.970434]\n",
      "198 [D loss: 1.642656, acc.: 42.19%] [G loss: 1.831249]\n",
      "199 [D loss: 0.910615, acc.: 62.50%] [G loss: 1.924973]\n"
     ]
    }
   ],
   "source": [
    "gan = GAN()\n",
    "gan.train(epochs=200, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
